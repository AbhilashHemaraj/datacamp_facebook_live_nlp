{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequencies of words in novels: a Data Science pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/moby-dick.jpg\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code-along session, you will use some basic Natural Language Processing to plot the most frequently occurring words in the novel _Moby Dick_. In doing so, you'll also see the efficacy of thinking in terms of the following Data Science pipeline with a constant regard for process:\n",
    "1. State your question;\n",
    "2. Get your data;\n",
    "3. Wrangle your data to answer your question;\n",
    "4. Answer your question;\n",
    "5. Present your solution so that others can understand it.\n",
    "\n",
    "For example, what would the following word frequency histogram be from?\n",
    "\n",
    "<img src=\"img/d-x.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-steps\n",
    "\n",
    "Follow the instructions in the README.md to get your system set up and ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. State your question\n",
    "\n",
    "What are the most frequent words in the novel _Moby Dick_ and how often do they occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your raw data is the text of Melville's novel _Moby Dick_. How would you go about getting the text of this ~800 word book into Python? Well, there are several ways to do this but first realise that the text is freely available online at [Project Gutenberg](https://www.gutenberg.org/) (contains a large corpus of books). Let's head there, try to find _Moby Dick_ and then store the relevant url in your Python namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the url, you need to fetch the HTML of the website. \n",
    "\n",
    "* Note that HTML stands for Hypertext Markup Language and is the standard markup language for the web.\n",
    "\n",
    "\n",
    "You're going to use [`requests`](http://docs.python-requests.org/en/master/) to do this, one of the [most popular](https://pythontips.com/2013/07/30/20-python-libraries-you-cant-live-without/) and useful Python packages out there.\n",
    "You can find out more in DataCamp's [Importing Data in Python (Part 2) course](https://www.datacamp.com/courses/importing-data-in-python-part-2). \n",
    "\n",
    "<img src=\"img/requests.png\" width=\"200\">\n",
    "\n",
    "According to the `requests` package website:\n",
    "\n",
    "> Requests allows you to send organic, grass-fed HTTP/1.1 requests, without the need for manual labor.\n",
    "\n",
    "and the following organizations claim to use `requests` internally:\n",
    "\n",
    "> Her Majesty's Government, Amazon, Google, Twilio, NPR, Obama for America,  Twitter, Sony, and Federal U.S. Institutions that prefer to be unnamed.\n",
    "\n",
    "Moreover,\n",
    "\n",
    "> Requests is one of the most downloaded Python packages of all time, pulling in over 13,000,000 downloads every month. All the cool kids are doing it!\n",
    "\n",
    "You'll be making a `GET` request from the website, which means you're _getting_ data from it. This is what you're doing through a browser when visiting a webpage using a browser. There are other types of requests, such as `POST` requests, but we won't concern ourselves with them here.\n",
    "\n",
    "`requests` make this easy with its `get` function. Make the request here and check the object type returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import `requests`\n",
    "\n",
    "\n",
    "# Make the request and check object type\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a `Response` object. You can see in the [`requests` kickstart guide](http://docs.python-requests.org/en/master/user/quickstart/) that a `Response` object has an attribute `text` that allows you to get the HTML from it! Let's do this and print the HTML to check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract HTML from Response object and print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! This HTML is not quite what you want. However, it does _contain_ what you want: the text of _Moby Dick_. What you need to do now is _wrangle_ this HTML to extract the novel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:** \n",
    "\n",
    "* you have now scraped the web to get _Moby Dick_ from Project Gutenberg.\n",
    "\n",
    "**Up next:** it's time for you to parse the html and extract the text of the novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrangle your data to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: getting the text from the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here you'll use the package [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/). The package website says:\n",
    "\n",
    "<img src=\"img/bs4.png\" width=\"550\">\n",
    "\n",
    "\n",
    "\n",
    "This looks promising!\n",
    "\n",
    "Firstly, a word on the name of the package: Beautiful Soup? In web development, the term \"tag soup\" refers to structurally or syntactically incorrect HTML code written for a web page. What Beautiful Soup does best is to make tag soup beautiful again and to extract information from it with ease! In fact, the main object created and queried when using this package is called `BeautifulSoup`. After creating the soup, we can use its `.get_text()` method to extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import BeautifulSoup from bs4\n",
    "\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these soup objects, you can extract all types of interesting information about the website you're scraping, such as title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get soup title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the title as a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get soup title as string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or all URLs found within a pageâ€™s < a > tags (hyperlinks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get hyperlinks from soup and check out first 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you want to do is to extract the text from the `soup` and there's a super helpful `.get_text()` method precisely for this. \n",
    "\n",
    "**Get the text, print it out and have a look at it. Is it what you want?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the text out of the soup and print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is now nearly what you want. it is the text of the novel with some unwanted stuff at the start and some unwanted stuff at the end. You could remove it, if you wanted, but this content is so much smaller in amount than the text of Moby Dick that, to a first approximation, it is fine to leave in and this will be the approach here. To get robust results, I'd suggest removing it.\n",
    "\n",
    "Now that you have the text of interest, it's time for you to count how many times each word appears and to plot the frequency histogram that you want: Natural Language Processing to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:** \n",
    "\n",
    "* you have scraped the web to get _Moby Dick_ from Project Gutenberg;\n",
    "* you have also now parsed the html and extracted the text of the novel.\n",
    "\n",
    "**Up next:** you'll use Natural Language Processing, tokenization and regular expressions to extract the list of words in _Moby Dick_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Extract words from your text using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now use `nltk`, the Natural Language Toolkit, to\n",
    "\n",
    "1. Tokenize the text (fancy term for splitting into tokens, such as words);\n",
    "2. Remove stopwords (words such as 'a' and 'the' that occur a great deal in ~ nearly all English language texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Tokenize\n",
    "\n",
    "You want to tokenize your text, that is, split it into a list a words. Essentially, you want to split off the parts off the text that are separated by whitespaces.\n",
    "\n",
    "To do this, you're going to use a powerful tool called _regular expressions_. A regular expression, or _regex_ for short, is a _sequence of characters_ that define a _search pattern_. They are notoriously confusing and best introduced by example.\n",
    "\n",
    "* You have the string 'peter piper picked a peck of pickled peppers' and you want to extract from the list of _all_ words in it that start with a 'p'. \n",
    "\n",
    "The regular expression that matches all words beginning with 'p' is 'p\\w+'. Let's unpack this: \n",
    "\n",
    "* the 'p' at the beginning of the regular expression means that you'll only match sequences of characters that start with a 'p';\n",
    "* the '\\w' is a special character that will match any alphanumeric A-Z, a-z, 0-9, along with underscores;\n",
    "* The '+' tells you that the previous character in the regex can appear as many times as you want in strings that you;re trying to match. This means that '\\w+' will match arbitrary sequences of alphanumeric characters and underscores.\n",
    "\n",
    "Put this all together and the regular expression 'p\\w+' will match all substrings that start with a 'p' and are followed by alphanumeric characters and underscores. In most English language texts that make sense, this will correspond to words beginning with 'p'.\n",
    "\n",
    "**You'll now use the built-in Python package `re` to extract all words beginning with 'p' from the sentence 'peter piper picked a peck of pickled peppers' as a warm-up.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import regex package\n",
    "\n",
    "\n",
    "\n",
    "# Define sentence\n",
    "\n",
    "\n",
    "# Define regex\n",
    "\n",
    "\n",
    "\n",
    "# Find all words in sentence that match the regex and print them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good. Now, if 'p\\w+' is the regex that matches words beginning with 'p', what's the regex that matches all words?\n",
    "\n",
    "**It's your job to now do this for our toy Peter Piper sentence above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all words and print them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can do the same with `text`, the string that contains _Moby Dick_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all words in Moby Dick and print several\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is also a way to do this with nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import RegexpTokenizer from nltk.tokenize\n",
    "\n",
    "\n",
    "# Create tokenizer\n",
    "\n",
    "\n",
    "\n",
    "# Create tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! You're nearly there. Note, though, that in the above, 'Or' has a capital 'O' and that in other places it may not but both 'Or' and 'or' you will want to count as the same word. For this reason, you will need to build a list of all words in _Moby Dick_ in which all capital letters have been made lower case. You'll find the string method `.lower()` handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize new list\n",
    "\n",
    "\n",
    "# Loop through list tokens and make lower case\n",
    "\n",
    "\n",
    "\n",
    "# Print several items from list as sanity check\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:** \n",
    "\n",
    "* you have scraped the web to get _Moby Dick_ from Project Gutenberg;\n",
    "* you have parsed the html and extracted the text of the novel;\n",
    "* you have used tokenization and regular expressions to extract the list of words in _Moby Dick_.\n",
    "\n",
    "**Up next:** remove common words such as 'a' and 'the' from the list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Remove stop words\n",
    "\n",
    "It is common practice to remove words that appear alot in the English language such as 'the', 'of' and 'a' (known as stopwords) because they're not so interesting. For more on all of these techniques, check out our [Natural Language Processing Fundamentals in Python course](https://www.datacamp.com/courses/nlp-fundamentals-in-python). \n",
    "\n",
    "The package `nltk` has a list of stopwords in English which you'll now store as `sw` and print the first several elements of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import stopwords from sklearn (on 2nd pass)\n",
    "\n",
    "\n",
    "# Import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Get English stopwords and print some of them\n",
    "\n",
    "\n",
    "# Add sklearn stopwords (2nd pass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want the list of all words in `words` that are *not* in `sw`. One way to get this list is to loop over all elements of `words` and add the to a new list if they are *not* in `sw`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize new list\n",
    "\n",
    "\n",
    "\n",
    "# Add to words_ns all words that are in words but not in sw\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print several list items as sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer your question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Our question was 'What are the most frequent words in the novel Moby Dick and how often do they occur?' \n",
    "\n",
    "You can now plot a frequency histogram of words in Moby Dick in two line of code using `nltk`. To do this,\n",
    "\n",
    "* Create a frequency distribution object using the function `nltk.FreqDist()`;\n",
    "* Using the plot method of the resulting object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import datavis libraries\n",
    "\n",
    "\n",
    "\n",
    "# Figures inline and set visualization style\n",
    "\n",
    "\n",
    "\n",
    "# Create freq hist and plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word 'one' is the second most common word in _Moby Dick_. Perhaps you would also like 'one' to be a stop word, along with a bunch of other words. Below, you'll add more stop words to your stop word list using a set of stop words from scikit-learn. You'll then re-plot the word frequency histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import stopwords from sklearn\n",
    "\n",
    "\n",
    "# Add sklearn stopwords to words_sw\n",
    "\n",
    "\n",
    "# Initialize new list\n",
    "\n",
    "\n",
    "# Add to words_ns all words that are in words but not in sw\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print several list items as sanity check\n",
    "\n",
    "# Create freq hist and plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Present your solution so that others can understand it.\n",
    "\n",
    "The cool thing is that, in using `nltk` to answer our question, we actually already presented our solution in a manner that can be communicated to other: a frequency distribution plot! You can read off the most common words, along with their frequency. For example, 'whale' is the most common word in the novel (go figure), excepting stopwords, and it occurs a whopping >1200 times! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## BONUS MATERIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen that there are lots of novels on Project Gutenberg we can make these word frequency histograms of, it makes sense to write your own function that does all of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
